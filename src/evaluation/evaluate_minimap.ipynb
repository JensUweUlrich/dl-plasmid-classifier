{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "from evaluation_helper import get_time_and_rss, get_max_gpu_usage"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "guppy_log_dir = '../../logs/guppy'\n",
    "minimap_log_dir = '../../logs/minimap'\n",
    "res_dir = '../../data/eval'\n",
    "minimap_res_dir = '../../data/eval/minimap'"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "measures = pd.DataFrame()\n",
    "for ds in ['real', 'sim']:\n",
    "    for mx in [4, 6, 8]:\n",
    "        # add guppy results\n",
    "        guppy_logfile_time = f'{guppy_log_dir}/step6c_basecall_{ds}_{mx}.txt'\n",
    "        guppy_logfile_nvidia = f'{guppy_log_dir}/step6c_basecall_{ds}_{mx}_gpu.txt'\n",
    "        g_user_time, g_system_time, g_elapsed_time, g_max_rss = get_time_and_rss(guppy_logfile_time)\n",
    "        max_gpu_usage = get_max_gpu_usage(guppy_logfile_nvidia, 'guppy_basecaller')\n",
    "        measures = pd.concat([measures, pd.DataFrame([{'Approach': 'Guppy',\n",
    "                                                       'Dataset': ds,\n",
    "                                                       'Maximum Sequence Length': mx * 1000,\n",
    "                                                       'User Time': g_user_time,\n",
    "                                                       'System Time': g_system_time,\n",
    "                                                       'Elapsed Time': g_elapsed_time,\n",
    "                                                       'Max RSS (GB)': g_max_rss,\n",
    "                                                       'Max GPU Memory Usage (GiB)': max_gpu_usage}])], ignore_index=True)\n",
    "\n",
    "        # add minimap results\n",
    "        minimap_logfile_time = f'{minimap_log_dir}/step6e_map_{ds}_{mx}.txt'\n",
    "        m_user_time, m_system_time, m_elapsed_time, m_max_rss = get_time_and_rss(minimap_logfile_time)\n",
    "        measures = pd.concat([measures, pd.DataFrame([{'ID': f'max{mx}_{ds}',\n",
    "                                                       'Approach': 'Minimap',\n",
    "                                                       'Dataset': ds,\n",
    "                                                       'Maximum Sequence Length': mx * 1000,\n",
    "                                                       'User Time': m_user_time,\n",
    "                                                       'System Time': m_system_time,\n",
    "                                                       'Elapsed Time': m_elapsed_time,\n",
    "                                                       'Max RSS (GB)': m_max_rss,\n",
    "                                                       'Max GPU Memory Usage (GiB)': 0.0}])], ignore_index=True)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "measures['User Time'] = pd.to_timedelta(measures['User Time'])\n",
    "measures['System Time'] = pd.to_timedelta(measures['System Time'])\n",
    "measures['Elapsed Time'] = '00:' + measures['Elapsed Time']\n",
    "measures['Elapsed Time'] = pd.to_timedelta(measures['Elapsed Time'])"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "summed_measures = measures.groupby(['Dataset', 'Maximum Sequence Length'])['User Time', 'System Time', 'Elapsed Time', 'Max RSS (GB)', 'Max GPU Memory Usage (GiB)'].apply(lambda x : x.sum())\n",
    "summed_measures = summed_measures.reset_index()\n",
    "summed_measures['Approach'] = 'Guppy + Minimap'\n",
    "measures = pd.concat([measures, summed_measures], ignore_index=True)\n",
    "measures"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [],
   "source": [
    "measures.to_csv(f'{res_dir}/MINIMAP_times_and_measures.csv', index=False)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def extract_ref_names(ref_path, txt_path):\n",
    "    with open(ref_path, 'r') as f_in, open(txt_path, 'w') as f_out:\n",
    "        for line in f_in.readlines():\n",
    "            if \">\" in line:\n",
    "                f_out.write(f'{line.split(\" \")[0][1:]}\\n')\n",
    "\n",
    "extract_ref_names(f'{minimap_res_dir}/real_pos_references.fasta', f'{minimap_res_dir}/real_pos_reference_names.txt')\n",
    "extract_ref_names(f'{minimap_res_dir}/real_neg_references.fasta', f'{minimap_res_dir}/real_neg_reference_names.txt')\n",
    "extract_ref_names(f'{minimap_res_dir}/sim_pos_references.fasta', f'{minimap_res_dir}/sim_pos_reference_names.txt')\n",
    "extract_ref_names(f'{minimap_res_dir}/sim_neg_references.fasta', f'{minimap_res_dir}/sim_neg_reference_names.txt')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def assign_class_to_ref(ref_name, pos_refs, neg_refs):\n",
    "    if ref_name in pos_refs:\n",
    "        return 'pos'\n",
    "    elif ref_name in neg_refs:\n",
    "        return 'neg'\n",
    "    else:\n",
    "        raise ValueError(f'Reference name \"{ref_name}\" not known!')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "metrics = pd.DataFrame(columns=['ID', 'Maximum Sequence Length', 'Dataset', 'TP', 'TN', 'FP', 'FN', 'UP', 'UC'])\n",
    "\n",
    "for ds in ['real', 'sim']:\n",
    "    neg_refs = open(f'{minimap_res_dir}/{ds}_neg_reference_names.txt').read().splitlines()\n",
    "    pos_refs = open(f'{minimap_res_dir}/{ds}_pos_reference_names.txt').read().splitlines()\n",
    "\n",
    "    for mx in [4, 6, 8]:\n",
    "        merged = pd.DataFrame()\n",
    "        unclassified_plasmids, unclassified_chromosomes = 0, 0\n",
    "        ids = pd.read_csv(f'{res_dir}/max{mx}_gt_test_{ds}_labels.csv')\n",
    "\n",
    "        for cls in ['pos', 'neg']:\n",
    "            alignment = pd.read_csv(f'{minimap_res_dir}/{ds}_max{mx}/{cls}_reads_and_refs.csv', sep='\\t', names=['Read', 'Reference', 'Quality'])\n",
    "            alignment['Read Class'] = cls  # ground truth label\n",
    "            alignment['Reference Class'] = alignment['Reference'].apply(lambda ref: assign_class_to_ref(ref, pos_refs, neg_refs))  # predicted label\n",
    "            merged = pd.concat([merged, alignment], ignore_index=True)\n",
    "\n",
    "            all_reads = len(ids[ids['GT Label'] == f'{\"plasmid\" if cls == \"pos\" else \"chr\"}'])\n",
    "            matched_reads = len(alignment)\n",
    "            if cls == 'pos':\n",
    "                unclassified_plasmids = all_reads - matched_reads\n",
    "            else:\n",
    "                unclassified_chromosomes = all_reads - matched_reads\n",
    "\n",
    "        metrics = pd.concat([metrics, pd.DataFrame([{\n",
    "            'ID': f'max{mx}_{ds}',\n",
    "            'Maximum Sequence Length': int(mx) * 1000,\n",
    "            'Dataset': ds,\n",
    "            'TP': len(merged[(merged['Reference Class'] == 'pos') & (merged['Read Class'] == 'pos')]),\n",
    "            'TN': len(merged[(merged['Reference Class'] == 'neg') & (merged['Read Class'] == 'neg')]),\n",
    "            'FP': len(merged[(merged['Reference Class'] == 'pos') & (merged['Read Class'] == 'neg')]),\n",
    "            'FN': len(merged[(merged['Reference Class'] == 'neg') & (merged['Read Class'] == 'pos')]),\n",
    "            'UP': unclassified_plasmids,\n",
    "            'UC': unclassified_chromosomes,\n",
    "        }])], ignore_index=True)\n",
    "\n",
    "metrics['TPR (Sensitivity)'] = metrics['TP'] / (metrics['TP'] + metrics['FN'] + metrics['UP'])\n",
    "metrics['TNR (Specificity)'] = metrics['TN'] / (metrics['TN'] + metrics['FP'] + metrics['UC'])\n",
    "metrics['FPR'] = 1 - metrics['TNR (Specificity)']\n",
    "metrics['FNR'] = 1 - metrics['TPR (Sensitivity)']\n",
    "metrics['Balanced Accuracy'] = (metrics['TPR (Sensitivity)'] + metrics['TNR (Specificity)']) / 2\n",
    "metrics['Accuracy'] = (metrics['TP'] + metrics['TN']) / (metrics['TP'] + metrics['TN'] + metrics['FP'] + metrics['FN'] + metrics['UP'] + metrics['UC'])\n",
    "metrics"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [],
   "source": [
    "metrics.to_csv(f'{res_dir}/MINIMAP_metrics.csv', index=False)"
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
